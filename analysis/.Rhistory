dat <- dat[dat$species != "Strigops habroptila", ] # remove problematic species
int <- intersect(dat$species, tree$tip.label)
# dat <- dat[dat$species %in% int, ] not needed
pruned.tree <- keep.tip(tree, int)
# Pre-normalize main effect columns (for all predictors used in any candidate)
# Determine unique main effect names from blocks:
all_main <- unique(unlist(lapply(blocks, function(x) x[1:2])))
for (var in all_main) {
if(var %in% names(dat)) {
dat[[var]] <- (max(dat[[var]]) - dat[[var]]) / diff(range(dat[[var]]))
}
}
# --- Model Fitting in Parallel ---
# Define a function to process one candidate:
process_candidate <- function(candidate_info, candidate_index) {
terms <- candidate_info$predictors
main <- terms[!grepl(":", terms)]
interactions <- terms[grepl(":", terms)]
# Subset data (only main effects are needed)
dat_sub <- na.omit(dat[, c("species", "clade", "rsq", main)])
if(nrow(dat_sub) < 14) return(NULL)
# Fit candidate model using glm
mod <- tryCatch(
glm(reformulate(terms, response = "rsq"), data = dat_sub, na.action = na.fail),
error = function(e) {
message("Candidate ", candidate_index, " failed: ", e$message)
return(NULL)
}
)
if(is.null(mod)) return(NULL)
# Build constraints if any interactions exist
if(length(interactions) > 0) {
constraints <- sapply(interactions, function(int) {
parts <- strsplit(int, ":")[[1]]
# Wrap each predictor name in backticks
sprintf("((!`%s`) | (`%s` & `%s`))", int, parts[1], parts[2])
})
subset.expr <- parse(text = paste(constraints, collapse = " & "))[[1]]
} else {
subset.expr <- TRUE
}
1
# Run dredge
dredged <- dredge(mod, subset = subset.expr,
extra = list(shapirowilk.p = sw.test, lambda.p = lambda.test))
# Save output
saveRDS(dredged, paste0("../results/sauria.models/Sauria.", candidate_index, ".rds"))
return(candidate_index)
}
# Run in parallel using mclapply (adjust mc.cores as needed)
library(parallel)
ncores <- detectCores() - 8
cl <- makeCluster(ncores)
# Export necessary functions and objects
clusterExport(cl, varlist = c("candidate_list", "dat", "tree", "pruned.tree",
"sw.test", "lambda.test", "process_candidate", "blocks"),
envir = environment())
# Load required libraries on each worker
clusterEvalQ(cl, {
library(MuMIn)
library(phytools)
library(caper)
})
results <- parLapply(cl, seq_along(candidate_list), function(i) {
process_candidate(candidate_list[[i]], i)
})
stopCluster(cl)
cat("Finished processing candidates.\n")
library(MuMIn)
# columns in combined table
cols <- c("(Intercept)", "age.dna", "age.line", "age.ltr", "age.sine",
"age.unknown", "age.others", "prop.dna", "prop.line", "prop.ltr",
"prop.sine", "prop.unknown", "prop.others", "age.dna:prop.dna",
"age.line:prop.line", "age.ltr:prop.ltr", "age.sine:prop.sine",
"age.unknown:prop.unknown", "age.others:prop.others", "shapirowilk.p",
"lambda.p", "df", "logLik", "AICc", "delta", "weight")
# factor for attributes(model)$column.types
ctypes <- c(rep("terms", 19), rep("extra", 2), "df",
"loglik", "ic", "delta", "weight")
levels <- c("terms", "varying", "extra", "df",
"loglik", "ic", "delta", "weight")
ctypes <- factor(ctypes, levels = levels)
names(ctypes) <- cols
# vector for attribute(model)$terms
terms <- cols[1:19]
attributes(terms)$interceptLabel <- "(Intercept)"
# add missing columns and sort by the order of cols
fix_columns <- function(df, cols) {
miss <- setdiff(cols, colnames(df))
for(col in miss) df[[col]] <- NA
return(df[, cols, drop = FALSE])
}
combined.models <- NULL
combined.coefTables <- list()
for(i in 1:896){
if (!file.exists(paste0("../results/sauria.models/Sauria.", i, ".rds"))) {
next
}
cur.models <- readRDS(paste0("../results/sauria.models/Sauria.", i, ".rds"))
# fix attributes(cur.models)$column.types and $names
cur.attr <- attributes(cur.models)
cur.attr$column.types <- ctypes
cur.attr$names <- cols
# add missing columns and sort by the order of cols;
cur.models <- fix_columns(as.data.frame(cur.models), cols)
# assign unique indices to each model
cur.models$rn <- paste(i, rownames(cur.models), sep = ".")
# save attributes(cur.models)$coefTables and assign unique indices to each
cur.coefTables <- cur.attr$coefTables
names(cur.coefTables) <- paste(i, names(cur.coefTables), sep = ".")
# combine current and previous models
if (is.null(combined.models)) {
combined.models <- cur.models
} else {
combined.models <- rbind(combined.models, cur.models)
}
#combine current and previous attribute(cur.models)$coefTables
combined.coefTables <- c(combined.coefTables, cur.coefTables)
}
# reassign rownames
rownames(combined.models) <- combined.models$rn
combined.models$rn <- NULL
# remove duplicate models based on parameter inclusion
# to accomodate rounding errors, actual values are not matched
# models with the same parameter inclusion patterns have the same parameter estimates
pattern <- apply(combined.models[, 1:19], 1, function(x) {
paste(ifelse(is.na(x), "0", "1"), collapse = ".")
})
library(MuMIn)
# columns in combined table
cols <- c("(Intercept)", "age.dna", "age.line", "age.ltr", "age.sine",
"age.unknown", "age.others", "prop.dna", "prop.line", "prop.ltr",
"prop.sine", "prop.unknown", "prop.others", "age.dna:prop.dna",
"age.line:prop.line", "age.ltr:prop.ltr", "age.sine:prop.sine",
"age.unknown:prop.unknown", "age.others:prop.others", "shapirowilk.p",
"lambda.p", "df", "logLik", "AICc", "delta", "weight")
# factor for attributes(model)$column.types
ctypes <- c(rep("terms", 19), rep("extra", 2), "df",
"loglik", "ic", "delta", "weight")
levels <- c("terms", "varying", "extra", "df",
"loglik", "ic", "delta", "weight")
ctypes <- factor(ctypes, levels = levels)
names(ctypes) <- cols
# vector for attribute(model)$terms
terms <- cols[1:19]
attributes(terms)$interceptLabel <- "(Intercept)"
# add missing columns and sort by the order of cols
fix_columns <- function(df, cols) {
miss <- setdiff(cols, colnames(df))
for(col in miss) df[[col]] <- NA
return(df[, cols, drop = FALSE])
}
combined.models <- NULL
combined.coefTables <- list()
for(i in 1:896){
if (!file.exists(paste0("../results/sauria.models/Sauria.", i, ".rds"))) {
next
}
cur.models <- readRDS(paste0("../results/sauria.models/Sauria.", i, ".rds"))
# fix attributes(cur.models)$column.types and $names
cur.attr <- attributes(cur.models)
cur.attr$column.types <- ctypes
cur.attr$names <- cols
# add missing columns and sort by the order of cols;
cur.models <- fix_columns(as.data.frame(cur.models), cols)
# assign unique indices to each model
cur.models$rn <- paste(i, rownames(cur.models), sep = ".")
# save attributes(cur.models)$coefTables and assign unique indices to each
cur.coefTables <- cur.attr$coefTables
names(cur.coefTables) <- paste(i, names(cur.coefTables), sep = ".")
# combine current and previous models
if (is.null(combined.models)) {
combined.models <- cur.models
} else {
combined.models <- rbind(combined.models, cur.models)
}
#combine current and previous attribute(cur.models)$coefTables
combined.coefTables <- c(combined.coefTables, cur.coefTables)
}
# reassign rownames
rownames(combined.models) <- combined.models$rn
combined.models$rn <- NULL
# remove duplicate models based on parameter inclusion
# to accomodate rounding errors, actual values are not matched
# models with the same parameter inclusion patterns have the same parameter estimates
pattern <- apply(combined.models[, 1:19], 1, function(x) {
paste(ifelse(is.na(x), "0", "1"), collapse = ".")
})
final.model <- combined.models[!duplicated(pattern), ]
final.coefTables <- combined.coefTables[!duplicated(pattern)]
# reassign rownames
rownames(final.model) <- as.character(seq_len(nrow(final.model)))
names(final.coefTables) <- as.character(seq_len(nrow(final.model)))
# set other attributes
attributes(final.model)$model.calls <- NULL
attributes(final.model)$coefTables <- final.coefTables
attributes(final.model)$column.types <- ctypes
attributes(final.model)$names <- cols
attributes(final.model)$class <- c("model.selection", "data.frame")
attributes(final.model)$terms <- terms
# reorder and save
final.model <- final.model[order(final.model$AICc), ]
saveRDS(final.model, "../results/sauria.09.rds")
View(final.model)
library(MuMIn)
library(viridis)
all <- readRDS("../results/all.09.rds")
mam <- readRDS("../results/mammalia.09.rds")
fish <- readRDS("../results/actinopterygii.09.rds")
rep <- readRDS("../results/sauria.09.rds")
all <- all[order(all$AICc), ]
mam <- mam[order(mam$AICc), ]
fish <- fish[order(fish$AICc), ]
rep <- rep[order(rep$AICc), ]
# recalculate reptile AICc and remove models that seem overparametrized
rep <- rbind(rep, rep[1,])
rep <- rep[-1,]
rep <- rep[rep$AICc > -200, ]
all <- all[cumsum(all$weight) <= 0.95, ]
mam <- mam[cumsum(mam$weight) <= 0.95, ]
fish <- fish[cumsum(fish$weight) <= 0.95, ]
rep <- rep[cumsum(rep$weight) <= 0.95, ]
combined.df <- data.frame()
for (i in c("all", "mam", "fish", "rep")) {
assign(i, model.avg(get(i))) # average
imp <- sort(sw(get(i)), decreasing = TRUE) # get importance
ci <- confint(get(i)) # get ci
ci <- ci[match(names(imp), row.names(ci)), ] #match ci
ci <- as.data.frame(ci)
idx <- which(sign(ci[, 1]) == sign(ci[, 2])) # idx where 0 is not in ci
ci <- ci[idx, ]# subset ci
imp <- imp[idx]# subset importance
df <- data.frame(i,
names(imp),
sapply(1:nrow(ci), function(x) mean(unlist(ci[x, ]))),
imp,
ci[, 1],
ci[, 2])
colnames(df) <- c("clade", "model", "estimate", "importance", "lower", "upper")
if (is.null(combined.df)) {
df <- combined.df
} else {
combined.df <- rbind(combined.df, df)
}
}
combined.df <- combined.df[combined.df$importance >= 0.5, ]
# x positions
x <- c()
for (i in 1:length(combined.df$clade)) {
if (is.null(x)) {
x <- c(1)
} else {
x <- c(x, ifelse(combined.df$clade[i] == prev, tail(x, 1) + 0.55, tail(x, 1) + 1))
}
prev <- combined.df$clade[i]
}
imp <- combined.df$importance
# color mapping
res <- 10000 # resolution
palette <- viridis(res, begin = 0, end = 0.8, option = "A") # palette
cols <- palette[round(((imp - min(imp)) / diff(range(imp))) * (res-1)) + 1] # colors
# x labels
labels <- c()
for (i in combined.df$model) {
rep <- toupper(regmatches(i, regexpr("(?<=\\.)[a-zA-Z]+", i, perl = TRUE)))
if (rep == "OTHERS") {
rep <- "Others"
} else if (rep == "UNKNOWN") {
rep <- "Unidtf"
}
if (grepl(":", i)) {
type <- "int."
} else if (sub("\\..*", "", i) == "prop") {
type <- "prop."
} else {
type <- "age"
}
labels <- c(labels, paste(rep, type))
}
par(oma = c(0, 0, 3, 0))
layout(matrix(1:2, ncol = 2), widths = c(4, 1)) # make 2 plots
# main plot
par(mar = c(8, 4, 1, 0))
int.range <- range(as.matrix(combined.df[c("lower", "upper")]))
plot(y = combined.df$estimate, x = x, type = "n", ylim = 1.05 * int.range,
ylab = "Parameter estimate", xlab = NA, axes = FALSE,
xlim = c(min(x)-0.25, max(x)+0.25)) # plot
abline(h = 0, lty = 1, col = "black") # line at y = 0
for (l in -100:100) {
abline(h = l, lty = 2, col = "grey") # line at y = 0
}
segments(x, combined.df$lower, x, combined.df$upper, lwd = 2) # confidence bars
segments(x-0.1, combined.df$upper, x+0.1, combined.df$upper, lwd = 2)
segments(x-0.1, combined.df$lower, x+0.1, combined.df$lower, lwd = 2)
points(x, combined.df$estimate, pch = 16, cex = 1.5, col = cols) # colored points
axis(2) # y axis
axis(2, at = seq(-10, 10, by = 0.5), labels = FALSE, tcl = -0.2)
axis(2, at = seq(-10, 10, by = 1), labels = FALSE, tcl = -0.5)
axis(1, at = x, labels = labels, las = 2) # x axis
box()
# color bar
par(mar = c(8, 1, 1, 4))
height <- seq(min(imp), max(imp), length.out = res + 1) # y values
z <- matrix(seq(min(imp), max(imp), length.out = res), nrow = 1, ncol = res) # color gradient
image(x = c(0, 1), y = height, z = z, col = palette,
axes = FALSE, xlab = "", ylab = "") # make color bar
ticks <- seq(min(imp), max(imp), length.out = 5) # ticks
axis(4, at = ticks, labels = round(ticks, 2), las = 1) # y axis
# title
mtext("Parameter estimates for averaged models",
outer = TRUE, cex = 1.1, line = 0, font = 2, family = "sans",
adj = 0.35)
par(mar = c(5, 4, 4, 2) + 0.1)
# Zhaobo Hu
# zhaobohu2002@gmail.com
# Description: Parses results and calculates additional statistics
# to summarize contigs for each species
dat <- read.csv("../data/data.csv")
# combine raw contig results
library(data.table)
dir <- "../results/indiv.contigs"
files <- paste0(dir, "/",  list.files(dir))
contigs <- lapply(files, fread)
contigs <- as.data.frame(rbindlist((contigs), fill = TRUE))
# parse by contig size
contigs <- contigs[contigs$size.Mb >= 10, ]
# remove species with less than 3 contigs
rm <- names(table(contigs$species)[table(contigs$species) < 3])
contigs <- contigs[!(contigs$species %in% rm), ]
df <- data.frame()
# contig sum/assembly size ratio threshold
# for (thrs in seq(from = 0, to = 1, by = 0.01)) {
for (thrs in c(0.8)) {
# for (thrs in c(0.95)) {
# remove species if sum of contig sizes is not within some multiple of assembly size
parsed <- data.frame()
for (z in unique(contigs$species)) {
sub <- contigs[contigs$species == z, ]
cont <- sum(sub$size.Mb)
total <- contigs[contigs$species == z, ]$asmblysize[1]
if (cont/total >= thrs) {
parsed <- rbind(parsed, sub)
}
}
# calculate stats based on parsed results
sp <- unique(parsed$species)
for (species in sp) {
sub <- parsed[which(parsed$species == species), ]
# fit <- summary(glm(sub$genecount ~ sub$size.Mb))
# beta <- fit$coefficients[2, 1]
# pval.beta <- fit$coefficients[2, 4]
rsq <- summary(lm(sub$genecount ~ sub$size.Mb))$r.squared
# weightmean <- sum(sub$genedens * sub$size.Mb) / sum(sub$size.Mb)
# weightsd <- sqrt(sum(sub$size.Mb * (sub$genedens - weightmean)^2) / sum(sub$size.Mb))
# weightcv <- weightsd / weightmean
stats <- data.frame(species, rsq, thrs)
sub <- merge(sub, stats, by = "species", all = TRUE)
sub <- merge(dat[dat$species == species, ], sub, by = "species", all = TRUE)
df <- rbind(df, sub)
}
}
#assign clades
df$clade <- df$class
df[df$clade %in% "Aves", ]$clade <- "Sauria"
df[df$clade %in% "Reptilia", ]$clade <- "Sauria"
df[!(df$clade %in% c("Actinopterygii", "Mammalia", "Sauria")), ]$clade <- "Others"
# reorder columns
df <- df[, c(18, 1, 19, 2:11, 16:17, 12:15)]
# write csv
write.csv(df, "../results/parsed.09.csv", row.names = FALSE)
# Zhaobo Hu
# zhaobohu2002@gmail.com
# Description: Parses results and calculates additional statistics
# to summarize contigs for each species
dat <- read.csv("../data/data.csv")
# combine raw contig results
library(data.table)
dir <- "../results/indiv.contigs"
files <- paste0(dir, "/",  list.files(dir))
contigs <- lapply(files, fread)
contigs <- as.data.frame(rbindlist((contigs), fill = TRUE))
# parse by contig size
contigs <- contigs[contigs$size.Mb >= 10, ]
# remove species with less than 3 contigs
rm <- names(table(contigs$species)[table(contigs$species) < 3])
contigs <- contigs[!(contigs$species %in% rm), ]
df <- data.frame()
# contig sum/assembly size ratio threshold
# for (thrs in seq(from = 0, to = 1, by = 0.01)) {
for (thrs in c(0.9)) {
# for (thrs in c(0.95)) {
# remove species if sum of contig sizes is not within some multiple of assembly size
parsed <- data.frame()
for (z in unique(contigs$species)) {
sub <- contigs[contigs$species == z, ]
cont <- sum(sub$size.Mb)
total <- contigs[contigs$species == z, ]$asmblysize[1]
if (cont/total >= thrs) {
parsed <- rbind(parsed, sub)
}
}
# calculate stats based on parsed results
sp <- unique(parsed$species)
for (species in sp) {
sub <- parsed[which(parsed$species == species), ]
# fit <- summary(glm(sub$genecount ~ sub$size.Mb))
# beta <- fit$coefficients[2, 1]
# pval.beta <- fit$coefficients[2, 4]
rsq <- summary(lm(sub$genecount ~ sub$size.Mb))$r.squared
# weightmean <- sum(sub$genedens * sub$size.Mb) / sum(sub$size.Mb)
# weightsd <- sqrt(sum(sub$size.Mb * (sub$genedens - weightmean)^2) / sum(sub$size.Mb))
# weightcv <- weightsd / weightmean
stats <- data.frame(species, rsq, thrs)
sub <- merge(sub, stats, by = "species", all = TRUE)
sub <- merge(dat[dat$species == species, ], sub, by = "species", all = TRUE)
df <- rbind(df, sub)
}
}
#assign clades
df$clade <- df$class
df[df$clade %in% "Aves", ]$clade <- "Sauria"
df[df$clade %in% "Reptilia", ]$clade <- "Sauria"
df[!(df$clade %in% c("Actinopterygii", "Mammalia", "Sauria")), ]$clade <- "Others"
# reorder columns
df <- df[, c(18, 1, 19, 2:11, 16:17, 12:15)]
# write csv
write.csv(df, "../results/parsed.09.csv", row.names = FALSE)
dat <- read.csv("../results/parsed.09.csv")
files <- list.files("../results/divsums")
sp <- gsub("_", " ", gsub(".divsum$", "", files))
asmbsz <- dat[!duplicated(dat$species), ]
asmbsz <- asmbsz[asmbsz$species %in% sp, ]
asmbsz <- setNames(asmbsz$asmblysize.Mb*1000000, asmbsz$species)
repstats <- data.frame()
lis <- vector("list", length(sp))
for (h in 1:length(sp)) {
species <- sp[h]
# read text file into lines
divsum <- readLines(paste0("../results/divsums/", files[h]))
# look for the start of relevant information
phrase <- "Coverage for each repeat class and divergence (Kimura)"
start.index <- match(phrase, divsum) + 1
# condense relevant lines into a table
divsum <- divsum[start.index:length(divsum)]
divsum <- read.table(textConnection(divsum),
sep = " ",
header = TRUE)
# drop columns with all NA
divsum <- divsum[-c(which(sapply(divsum, function(col) all(is.na(col)))))]
lis[[h]] <- colnames(divsum)
}
to.collapse <- sort(unique(
sub("\\..*", "", unique(grep("\\.", unlist(lis), value = TRUE)))
))
others <- unique(grep("\\.", unlist(lis), value = TRUE, invert = TRUE))
others <- others[!others %in% to.collapse]
others <- others[!others %in% "Div"]
for (i in 1:length(sp)) {
species <- sp[i]
# read text file into lines
divsum <- readLines(paste0("../results/divsums/", files[i]))
# look for the start of relevant information
phrase <- "Coverage for each repeat class and divergence (Kimura)"
start.index <- match(phrase, divsum) + 1
# condense relevant lines into a table
divsum <- divsum[start.index:length(divsum)]
divsum <- read.table(textConnection(divsum),
sep = " ",
header = TRUE)
# drop columns with all NA
divsum <- divsum[-c(which(sapply(divsum, function(col) all(is.na(col)))))]
# collapse
for (j in to.collapse) {
pat <- paste0("^", j, "(\\.|$)")
headers <- grep(pat, names(divsum), value = TRUE)
sub <- divsum[, headers]
sums <- rowSums(as.matrix(sub))
#divsum <- divsum[, !names(divsum) %in% headers]
assign(j, sums)
}
# record others
for (k in others) {
pat <- paste0("^", k, "(\\.|$)")
headers <- grep(pat, names(divsum), value = TRUE)
sub <- divsum[, headers]
sums <- rowSums(as.matrix(sub))
assign(k, sums)
}
div <- divsum$Div
divsum <- data.frame(div, mget(to.collapse), mget(others))
# collapse further
to.keep <- c("div", "DNA", "LINE", "LTR", "SINE", "Unknown")
to.sum <- divsum[, colnames(divsum)[!colnames(divsum) %in% to.keep]]
divsum <- divsum[, to.keep]
divsum$others <- rowSums(to.sum)
df <- data.frame(species)
for (l in colnames(divsum)[-1]) {
# proportion
df[[paste0("prop.", tolower(l))]] <- sum(divsum[l] / asmbsz[sp[i]])
# age
df[[paste0("age.", tolower(l))]] <- which(cumsum(divsum[l]) > sum(divsum[l])/2)[1]
}
repstats <- rbind(repstats, df)
}
# reorganize and save results
dat <- merge(dat, repstats, by= "species", all = T)
dat <- dat[order(dat$size.Mb, decreasing = TRUE), ]
dat <- dat[order(dat$species), ]
dat <- dat[order(dat$thrs), ]
write.csv(dat,
"../results/parsed.09.csv",
row.names = FALSE)
