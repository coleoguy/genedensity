predictors = candidate_terms,
count = total_count,
config = grid[i, ]
)
}
}
cat("Total candidate models generated (with <=12 predictors):", length(candidate_list), "\n")
# --- Pre-read and preprocess tree/data ---
tree <- read.tree("../data/formatted.tree.nwk")
tree$tip.label <- gsub("_", " ", tree$tip.label)
dat <- read.csv("../results/parsed.csv")
dat <- dat[!is.na(dat$chromnum.1n) & !duplicated(dat$species), ]
dat <- dat[dat$clade %in% "Sauria", ]
int <- intersect(dat$species, tree$tip.label)
# dat <- dat[dat$species %in% int, ] not needed
pruned.tree <- keep.tip(tree, int)
# Pre-normalize main effect columns (for all predictors used in any candidate)
# Determine unique main effect names from blocks:
all_main <- unique(unlist(lapply(blocks, function(x) x[1:2])))
for (var in all_main) {
if(var %in% names(dat)) {
dat[[var]] <- (max(dat[[var]]) - dat[[var]]) / diff(range(dat[[var]]))
}
}
View(dat)
library(MuMIn)
library(phytools)
library(caper)
library(parallel)
options(na.action = "na.fail")
sw.test <- function(model) {
res <- residuals(model)
shapiro.test(res)$p.value
}
lambda.test <- function(model) {
# Check degrees of freedom: model$df.residual is available for glm objects
if(model$df.residual < 2) {
return(NA)
}
res <- setNames(residuals(model), dat$species)
phylosig(tree, res, method = "lambda", test = TRUE, niter = 100)$P
}
# Define your blocks as before:
blocks <- list(
dna     = c("age.dna",     "prop.dna",     "age.dna:prop.dna"),
line    = c("age.line",    "prop.line",    "age.line:prop.line"),
ltr     = c("age.ltr",     "prop.ltr",     "age.ltr:prop.ltr"),
sine    = c("age.sine",    "prop.sine",    "age.sine:prop.sine"),
unknown = c("age.unknown", "prop.unknown", "age.unknown:prop.unknown"),
others  = c("age.others",  "prop.others",  "age.others:prop.others")
)
# For each block, define the five possible states.
# Each state is a list with:
#   $terms: the predictors included in that state
#   $count: the number of predictors added
block_states <- list()
for (b in names(blocks)) {
block_states[[b]] <- list(
omit       = list(terms = character(0),         count = 0),
M1         = list(terms = blocks[[b]][1],         count = 1),
M2         = list(terms = blocks[[b]][2],         count = 1),
both       = list(terms = blocks[[b]][1:2],       count = 2),
both_int   = list(terms = blocks[[b]],            count = 3)
)
}
# Create the full combinatorial grid across blocks.
# This grid will have 5^6 = 15625 rows.
state_names <- lapply(block_states, names)  # each block's possible state names
grid <- expand.grid(state_names, stringsAsFactors = FALSE)
# Now, generate candidate models by combining the chosen states for each block.
candidate_list <- list()
for (i in 1:nrow(grid)) {
candidate_terms <- character(0)
total_count <- 0
for (b in names(blocks)) {
state <- grid[i, b]  # e.g., "omit", "M1", etc.
candidate_terms <- c(candidate_terms, block_states[[b]][[state]]$terms)
total_count <- total_count + block_states[[b]][[state]]$count
}
# Keep only candidates with a total of 12 or fewer predictors.
if (total_count == 13) {
candidate_list[[length(candidate_list) + 1]] <- list(
predictors = candidate_terms,
count = total_count,
config = grid[i, ]
)
}
}
cat("Total candidate models generated (with <=12 predictors):", length(candidate_list), "\n")
# --- Pre-read and preprocess tree/data ---
tree <- read.tree("../data/formatted.tree.nwk")
tree$tip.label <- gsub("_", " ", tree$tip.label)
dat <- read.csv("../results/parsed.csv")
dat <- dat[!is.na(dat$chromnum.1n) & !duplicated(dat$species), ]
dat <- dat[dat$clade %in% "Sauria", ]
int <- intersect(dat$species, tree$tip.label)
# dat <- dat[dat$species %in% int, ] not needed
pruned.tree <- keep.tip(tree, int)
View(dat)
View(dat)
View(dat)
dat <- dat[dat$species != "Strigops habroptila", ]
library(MuMIn)
library(phytools)
library(caper)
library(parallel)
options(na.action = "na.fail")
sw.test <- function(model) {
res <- residuals(model)
shapiro.test(res)$p.value
}
lambda.test <- function(model) {
# Check degrees of freedom: model$df.residual is available for glm objects
if(model$df.residual < 2) {
return(NA)
}
res <- setNames(residuals(model), dat$species)
phylosig(tree, res, method = "lambda", test = TRUE, niter = 100)$P
}
# Define your blocks as before:
blocks <- list(
dna     = c("age.dna",     "prop.dna",     "age.dna:prop.dna"),
line    = c("age.line",    "prop.line",    "age.line:prop.line"),
ltr     = c("age.ltr",     "prop.ltr",     "age.ltr:prop.ltr"),
sine    = c("age.sine",    "prop.sine",    "age.sine:prop.sine"),
unknown = c("age.unknown", "prop.unknown", "age.unknown:prop.unknown"),
others  = c("age.others",  "prop.others",  "age.others:prop.others")
)
# For each block, define the five possible states.
# Each state is a list with:
#   $terms: the predictors included in that state
#   $count: the number of predictors added
block_states <- list()
for (b in names(blocks)) {
block_states[[b]] <- list(
omit       = list(terms = character(0),         count = 0),
M1         = list(terms = blocks[[b]][1],         count = 1),
M2         = list(terms = blocks[[b]][2],         count = 1),
both       = list(terms = blocks[[b]][1:2],       count = 2),
both_int   = list(terms = blocks[[b]],            count = 3)
)
}
# Create the full combinatorial grid across blocks.
# This grid will have 5^6 = 15625 rows.
state_names <- lapply(block_states, names)  # each block's possible state names
grid <- expand.grid(state_names, stringsAsFactors = FALSE)
# Now, generate candidate models by combining the chosen states for each block.
candidate_list <- list()
for (i in 1:nrow(grid)) {
candidate_terms <- character(0)
total_count <- 0
for (b in names(blocks)) {
state <- grid[i, b]  # e.g., "omit", "M1", etc.
candidate_terms <- c(candidate_terms, block_states[[b]][[state]]$terms)
total_count <- total_count + block_states[[b]][[state]]$count
}
# Keep only candidates with a total of 12 or fewer predictors.
if (total_count == 12) {
candidate_list[[length(candidate_list) + 1]] <- list(
predictors = candidate_terms,
count = total_count,
config = grid[i, ]
)
}
}
cat("Total candidate models generated (with <=12 predictors):", length(candidate_list), "\n")
# --- Pre-read and preprocess tree/data ---
tree <- read.tree("../data/formatted.tree.nwk")
tree$tip.label <- gsub("_", " ", tree$tip.label)
dat <- read.csv("../results/parsed.csv")
dat <- dat[!is.na(dat$chromnum.1n) & !duplicated(dat$species), ]
dat <- dat[dat$clade %in% "Sauria", ]
dat <- dat[dat$species != "Strigops habroptila", ] # remove problematic species
int <- intersect(dat$species, tree$tip.label)
# dat <- dat[dat$species %in% int, ] not needed
pruned.tree <- keep.tip(tree, int)
# Pre-normalize main effect columns (for all predictors used in any candidate)
# Determine unique main effect names from blocks:
all_main <- unique(unlist(lapply(blocks, function(x) x[1:2])))
for (var in all_main) {
if(var %in% names(dat)) {
dat[[var]] <- (max(dat[[var]]) - dat[[var]]) / diff(range(dat[[var]]))
}
}
# --- Model Fitting in Parallel ---
# Define a function to process one candidate:
process_candidate <- function(candidate_info, candidate_index) {
terms <- candidate_info$predictors
main <- terms[!grepl(":", terms)]
interactions <- terms[grepl(":", terms)]
# Subset data (only main effects are needed)
dat_sub <- na.omit(dat[, c("species", "clade", "rsq", main)])
if(nrow(dat_sub) < 14) return(NULL)
# Fit candidate model using glm
mod <- tryCatch(
glm(reformulate(terms, response = "rsq"), data = dat_sub, na.action = na.fail),
error = function(e) {
message("Candidate ", candidate_index, " failed: ", e$message)
return(NULL)
}
)
if(is.null(mod)) return(NULL)
# Build constraints if any interactions exist
if(length(interactions) > 0) {
constraints <- sapply(interactions, function(int) {
parts <- strsplit(int, ":")[[1]]
# Wrap each predictor name in backticks
sprintf("((!`%s`) | (`%s` & `%s`))", int, parts[1], parts[2])
})
subset.expr <- parse(text = paste(constraints, collapse = " & "))[[1]]
} else {
subset.expr <- TRUE
}
1
# Run dredge
dredged <- dredge(mod, subset = subset.expr,
extra = list(shapirowilk.p = sw.test, lambda.p = lambda.test))
# Save output
saveRDS(dredged, paste0("../results/sauria.models/Sauria.", candidate_index, ".rds"))
return(candidate_index)
}
# Run in parallel using mclapply (adjust mc.cores as needed)
library(parallel)
ncores <- detectCores() - 8
cl <- makeCluster(ncores)
# Export necessary functions and objects
clusterExport(cl, varlist = c("candidate_list", "dat", "tree", "pruned.tree",
"sw.test", "lambda.test", "process_candidate", "blocks"),
envir = environment())
# Load required libraries on each worker
clusterEvalQ(cl, {
library(MuMIn)
library(phytools)
library(caper)
})
results <- parLapply(cl, seq_along(candidate_list), function(i) {
process_candidate(candidate_list[[i]], i)
})
library(MuMIn)
library(phytools)
library(caper)
library(parallel)
options(na.action = "na.fail")
sw.test <- function(model) {
res <- residuals(model)
shapiro.test(res)$p.value
}
lambda.test <- function(model) {
# Check degrees of freedom: model$df.residual is available for glm objects
if(model$df.residual < 2) {
return(NA)
}
res <- setNames(residuals(model), dat$species)
phylosig(tree, res, method = "lambda", test = TRUE, niter = 100)$P
}
# Define your blocks as before:
blocks <- list(
dna     = c("age.dna",     "prop.dna",     "age.dna:prop.dna"),
line    = c("age.line",    "prop.line",    "age.line:prop.line"),
ltr     = c("age.ltr",     "prop.ltr",     "age.ltr:prop.ltr"),
sine    = c("age.sine",    "prop.sine",    "age.sine:prop.sine"),
unknown = c("age.unknown", "prop.unknown", "age.unknown:prop.unknown"),
others  = c("age.others",  "prop.others",  "age.others:prop.others")
)
# For each block, define the five possible states.
# Each state is a list with:
#   $terms: the predictors included in that state
#   $count: the number of predictors added
block_states <- list()
for (b in names(blocks)) {
block_states[[b]] <- list(
omit       = list(terms = character(0),         count = 0),
M1         = list(terms = blocks[[b]][1],         count = 1),
M2         = list(terms = blocks[[b]][2],         count = 1),
both       = list(terms = blocks[[b]][1:2],       count = 2),
both_int   = list(terms = blocks[[b]],            count = 3)
)
}
# Create the full combinatorial grid across blocks.
# This grid will have 5^6 = 15625 rows.
state_names <- lapply(block_states, names)  # each block's possible state names
grid <- expand.grid(state_names, stringsAsFactors = FALSE)
# Now, generate candidate models by combining the chosen states for each block.
candidate_list <- list()
for (i in 1:nrow(grid)) {
candidate_terms <- character(0)
total_count <- 0
for (b in names(blocks)) {
state <- grid[i, b]  # e.g., "omit", "M1", etc.
candidate_terms <- c(candidate_terms, block_states[[b]][[state]]$terms)
total_count <- total_count + block_states[[b]][[state]]$count
}
# Keep only candidates with a total of 12 or fewer predictors.
if (total_count == 12) {
candidate_list[[length(candidate_list) + 1]] <- list(
predictors = candidate_terms,
count = total_count,
config = grid[i, ]
)
}
}
cat("Total candidate models generated (with <=12 predictors):", length(candidate_list), "\n")
# --- Pre-read and preprocess tree/data ---
tree <- read.tree("../data/formatted.tree.nwk")
tree$tip.label <- gsub("_", " ", tree$tip.label)
dat <- read.csv("../results/parsed.csv")
dat <- dat[!is.na(dat$chromnum.1n) & !duplicated(dat$species), ]
dat <- dat[dat$clade %in% "Sauria", ]
dat <- dat[dat$species != "Strigops habroptila", ] # remove problematic species
int <- intersect(dat$species, tree$tip.label)
# dat <- dat[dat$species %in% int, ] not needed
pruned.tree <- keep.tip(tree, int)
# Pre-normalize main effect columns (for all predictors used in any candidate)
# Determine unique main effect names from blocks:
all_main <- unique(unlist(lapply(blocks, function(x) x[1:2])))
for (var in all_main) {
if(var %in% names(dat)) {
dat[[var]] <- (max(dat[[var]]) - dat[[var]]) / diff(range(dat[[var]]))
}
}
# --- Model Fitting in Parallel ---
# Define a function to process one candidate:
process_candidate <- function(candidate_info, candidate_index) {
terms <- candidate_info$predictors
main <- terms[!grepl(":", terms)]
interactions <- terms[grepl(":", terms)]
# Subset data (only main effects are needed)
dat_sub <- na.omit(dat[, c("species", "clade", "rsq", main)])
if(nrow(dat_sub) < 14) return(NULL)
# Fit candidate model using glm
mod <- tryCatch(
glm(reformulate(terms, response = "rsq"), data = dat_sub, na.action = na.fail),
error = function(e) {
message("Candidate ", candidate_index, " failed: ", e$message)
return(NULL)
}
)
if(is.null(mod)) return(NULL)
# Build constraints if any interactions exist
if(length(interactions) > 0) {
constraints <- sapply(interactions, function(int) {
parts <- strsplit(int, ":")[[1]]
# Wrap each predictor name in backticks
sprintf("((!`%s`) | (`%s` & `%s`))", int, parts[1], parts[2])
})
subset.expr <- parse(text = paste(constraints, collapse = " & "))[[1]]
} else {
subset.expr <- TRUE
}
1
# Run dredge
dredged <- dredge(mod, subset = subset.expr,
extra = list(shapirowilk.p = sw.test, lambda.p = lambda.test))
# Save output
saveRDS(dredged, paste0("../results/sauria.models/Sauria.", candidate_index, ".rds"))
return(candidate_index)
}
# Run in parallel using mclapply (adjust mc.cores as needed)
library(parallel)
ncores <- detectCores() - 8
cl <- makeCluster(ncores)
# Export necessary functions and objects
clusterExport(cl, varlist = c("candidate_list", "dat", "tree", "pruned.tree",
"sw.test", "lambda.test", "process_candidate", "blocks"),
envir = environment())
# Load required libraries on each worker
clusterEvalQ(cl, {
library(MuMIn)
library(phytools)
library(caper)
})
results <- parLapply(cl, seq_along(candidate_list), function(i) {
process_candidate(candidate_list[[i]], i)
})
stopCluster(cl)
cat("Finished processing candidates.\n")
library(MuMIn)
# columns in combined table
cols <- c("(Intercept)", "age.dna", "age.line", "age.ltr", "age.sine",
"age.unknown", "age.others", "prop.dna", "prop.line", "prop.ltr",
"prop.sine", "prop.unknown", "prop.others", "age.dna:prop.dna",
"age.line:prop.line", "age.ltr:prop.ltr", "age.sine:prop.sine",
"age.unknown:prop.unknown", "age.others:prop.others", "shapirowilk.p",
"lambda.p", "df", "logLik", "AICc", "delta", "weight")
# factor for attributes(model)$column.types
ctypes <- c(rep("terms", 19), rep("extra", 2), "df",
"loglik", "ic", "delta", "weight")
levels <- c("terms", "varying", "extra", "df",
"loglik", "ic", "delta", "weight")
ctypes <- factor(ctypes, levels = levels)
names(ctypes) <- cols
# vector for attribute(model)$terms
terms <- cols[1:19]
attributes(terms)$interceptLabel <- "(Intercept)"
# add missing columns and sort by the order of cols
fix_columns <- function(df, cols) {
miss <- setdiff(cols, colnames(df))
for(col in miss) df[[col]] <- NA
return(df[, cols, drop = FALSE])
}
combined.models <- NULL
combined.coefTables <- list()
for(i in 1:896){
if (!file.exists(paste0("../results/sauria.models/Sauria.", i, ".rds"))) {
next
}
cur.models <- readRDS(paste0("../results/sauria.models/Sauria.", i, ".rds"))
# fix attributes(cur.models)$column.types and $names
cur.attr <- attributes(cur.models)
cur.attr$column.types <- ctypes
cur.attr$names <- cols
# add missing columns and sort by the order of cols;
cur.models <- fix_columns(as.data.frame(cur.models), cols)
# assign unique indices to each model
cur.models$rn <- paste(i, rownames(cur.models), sep = ".")
# save attributes(cur.models)$coefTables and assign unique indices to each
cur.coefTables <- cur.attr$coefTables
names(cur.coefTables) <- paste(i, names(cur.coefTables), sep = ".")
# combine current and previous models
if (is.null(combined.models)) {
combined.models <- cur.models
} else {
combined.models <- rbind(combined.models, cur.models)
}
#combine current and previous attribute(cur.models)$coefTables
combined.coefTables <- c(combined.coefTables, cur.coefTables)
}
# reassign rownames
rownames(combined.models) <- combined.models$rn
combined.models$rn <- NULL
# remove duplicate models based on parameter inclusion
# to accomodate rounding errors, actual values are not matched
# models with the same parameter inclusion patterns have the same parameter estimates
pattern <- apply(combined.models[, 1:19], 1, function(x) {
paste(ifelse(is.na(x), "0", "1"), collapse = ".")
})
library(MuMIn)
# columns in combined table
cols <- c("(Intercept)", "age.dna", "age.line", "age.ltr", "age.sine",
"age.unknown", "age.others", "prop.dna", "prop.line", "prop.ltr",
"prop.sine", "prop.unknown", "prop.others", "age.dna:prop.dna",
"age.line:prop.line", "age.ltr:prop.ltr", "age.sine:prop.sine",
"age.unknown:prop.unknown", "age.others:prop.others", "shapirowilk.p",
"lambda.p", "df", "logLik", "AICc", "delta", "weight")
# factor for attributes(model)$column.types
ctypes <- c(rep("terms", 19), rep("extra", 2), "df",
"loglik", "ic", "delta", "weight")
levels <- c("terms", "varying", "extra", "df",
"loglik", "ic", "delta", "weight")
ctypes <- factor(ctypes, levels = levels)
names(ctypes) <- cols
# vector for attribute(model)$terms
terms <- cols[1:19]
attributes(terms)$interceptLabel <- "(Intercept)"
# add missing columns and sort by the order of cols
fix_columns <- function(df, cols) {
miss <- setdiff(cols, colnames(df))
for(col in miss) df[[col]] <- NA
return(df[, cols, drop = FALSE])
}
combined.models <- NULL
combined.coefTables <- list()
for(i in 1:896){
if (!file.exists(paste0("../results/sauria.models/Sauria.", i, ".rds"))) {
next
}
cur.models <- readRDS(paste0("../results/sauria.models/Sauria.", i, ".rds"))
# fix attributes(cur.models)$column.types and $names
cur.attr <- attributes(cur.models)
cur.attr$column.types <- ctypes
cur.attr$names <- cols
# add missing columns and sort by the order of cols;
cur.models <- fix_columns(as.data.frame(cur.models), cols)
# assign unique indices to each model
cur.models$rn <- paste(i, rownames(cur.models), sep = ".")
# save attributes(cur.models)$coefTables and assign unique indices to each
cur.coefTables <- cur.attr$coefTables
names(cur.coefTables) <- paste(i, names(cur.coefTables), sep = ".")
# combine current and previous models
if (is.null(combined.models)) {
combined.models <- cur.models
} else {
combined.models <- rbind(combined.models, cur.models)
}
#combine current and previous attribute(cur.models)$coefTables
combined.coefTables <- c(combined.coefTables, cur.coefTables)
}
# reassign rownames
rownames(combined.models) <- combined.models$rn
combined.models$rn <- NULL
# remove duplicate models based on parameter inclusion
# to accomodate rounding errors, actual values are not matched
# models with the same parameter inclusion patterns have the same parameter estimates
pattern <- apply(combined.models[, 1:19], 1, function(x) {
paste(ifelse(is.na(x), "0", "1"), collapse = ".")
})
final.model <- combined.models[!duplicated(pattern), ]
final.coefTables <- combined.coefTables[!duplicated(pattern)]
# reassign rownames
rownames(final.model) <- as.character(seq_len(nrow(final.model)))
names(final.coefTables) <- as.character(seq_len(nrow(final.model)))
# set other attributes
attributes(final.model)$model.calls <- NULL
attributes(final.model)$coefTables <- final.coefTables
attributes(final.model)$column.types <- ctypes
attributes(final.model)$names <- cols
attributes(final.model)$class <- c("model.selection", "data.frame")
attributes(final.model)$terms <- terms
# reorder and save
final.model <- final.model[order(final.model$AICc), ]
saveRDS(final.model, "../results/sauria.09.rds")
View(final.model)
