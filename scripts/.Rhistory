results <- expand.grid(mu_int = grid_int, sigma = grid_sigma)
results$loss <- NA_real_
for (i in seq_len(nrow(results))) {
results$loss[i] <- loss_for(
int_mean    = results$mu_int[i],
sigma_scale = results$sigma[i]
)
cat(sprintf("tested int=%.2f sigma=%.2f → loss=%.4f\n",
results$mu_int[i],
results$sigma[i],
results$loss[i]))
}
library(brms)
# 1) Your data
y_obs <- r2.new
x_obs <- r2.observed
# 2) Revised loss function
loss_for <- function(int_mean, sigma_scale) {
# Build priors, embedding the numeric hyperparameters as strings:
ps <- c(
prior_string(
paste0("normal(", round(int_mean,3), ", 0.15)"),
class = "Intercept"
),
prior(normal(1, 0.15), class = "b", coef = "R2_obs"),
prior_string(
paste0("student_t(3, 0, ", round(sigma_scale,3), ")"),
class = "sigma"
)
)
# Prior-only fit (very short)
fit <- brm(
R2_pred_base ~ 1 + R2_obs,
data         = data.frame(R2_obs = x_obs, R2_pred_base = y_obs),
prior        = ps,
sample_prior = "only",
chains       = 1,
iter         = 500,
refresh      = 0
)
# Draw 200 prior-predictive samples
pp <- posterior_predict(fit, draws = 200)
ysim <- as.vector(pp)
# Loss = squared difference of means + squared difference of 90% quantiles
(mean(ysim) - mean(y_obs))^2 +
(quantile(ysim, 0.9) - quantile(y_obs, 0.9))^2
}
# 3) Grid search
grid_int   <- seq(0, 1, by = 0.2)
grid_sigma <- seq(0.1, 1, by = 0.2)
results    <- expand.grid(mu_int = grid_int, sigma = grid_sigma)
results$loss <- NA_real_
for (i in seq_len(nrow(results))) {
im <- results$mu_int[i]
ss <- results$sigma[i]
results$loss[i] <- loss_for(int_mean = im, sigma_scale = ss)
message(sprintf("int=%.2f sigma=%.2f → loss=%.4f", im, ss, results$loss[i]))
}
library(brms)
# 1) Your data
y_obs <- r2.new
x_obs <- r2.observed
set.seed(123)
# 1) Define your grid
grid_int   <- seq(0.0, 1.0, by = 0.2)
grid_sigma <- seq(0.1, 1.0, by = 0.2)
results    <- expand.grid(mu_int = grid_int, sigma = grid_sigma)
results$loss <- NA_real_
# 2) Loss function that only uses base R
loss_for_fast <- function(int_mean, sigma_scale, x_obs, y_obs,
n_draws = 200) {
# draw parameters
alpha_draws <- rnorm(n_draws, mean = int_mean,   sd = 0.15)
beta_draws  <- rnorm(n_draws, mean = 1.0,        sd = 0.15)
sigma_draws <- abs(rt(n_draws, df = 3) * sigma_scale / sqrt(3 / 1))
# Student-t(3,0,s) via scale
# for each draw, simulate y_pred_i = α + β x_i + ε_i
# We'll vectorize: for each draw j, simulate residuals across all species
ysim <- vapply(seq_len(n_draws), function(j) {
alpha   <- alpha_draws[j]
beta    <- beta_draws[j]
sigma_j <- sigma_draws[j]
y_j     <- alpha + beta * x_obs + rnorm(length(x_obs), 0, sigma_j)
return(y_j)
}, numeric(length(x_obs)))
# flatten, compare distributions
ysim_all <- as.vector(ysim)
(mean(ysim_all) - mean(y_obs))^2 +
(quantile(ysim_all, 0.9) - quantile(y_obs, 0.9))^2
}
# 3) Run the fast grid
for (i in seq_len(nrow(results))) {
im <- results$mu_int[i]
ss <- results$sigma[i]
results$loss[i] <- loss_for_fast(im, ss, x_obs, y_obs)
}
best <- results[which.min(results$loss), ]
print(best)
# 4) Use those best hyperparameters for your priors (then call brm just once)
priors_auto <- c(
prior(normal(best$mu_int, 0.15),   class = "Intercept"),
prior(normal(1,         0.15),   class = "b", coef = "R2_obs"),
prior(student_t(3, 0, best$sigma), class = "sigma")
)
print(priors_auto)
# 5) (Optional) Quick R‐only prior predictive plot
pp_fast <- loss_for_fast(best$mu_int, best$sigma, x_obs, y_obs, n_draws = 500)
# After your grid‐search, you have:
mu_int      <- best$mu_int    # e.g. 0.4
sigma_scale <- best$sigma     # e.g. 0.3
x_obs       <- x_obs          # your r2.observed vector
y_obs       <- y_obs          # your r2.new vector
n_draws     <- 200            # number of prior‐predictive draws
# 1) Sample from your priors:
set.seed(555)
alpha_draws <- rnorm(n_draws, mean = mu_int, sd = 0.15)
beta_draws  <- rnorm(n_draws, mean = 1.0,    sd = 0.15)
# half‐Student‐t(3) scaled:
t_raw       <- rt(n_draws, df = 3)
sigma_draws <- abs(t_raw) * sigma_scale / sqrt(3/1)
# 2) Simulate y_pred for each draw and each species:
#    result is a n_draws × length(x_obs) matrix
y_sim <- sapply(seq_len(n_draws), function(j) {
a <- alpha_draws[j]
b <- beta_draws[j]
s <- sigma_draws[j]
a + b * x_obs + rnorm(length(x_obs), 0, s)
})
# 3) Plot prior‐predictive cloud vs. observed
matplot(
x    = x_obs,
y    = t(y_sim),          # transpose: columns = draws
type = "p",
pch  = 16,
col  = rgb(0,0,1,0.05),
xlab = "Observed R²",
ylab = "Simulated R² (prior)",
main = "Prior Predictive (auto‐tuned hyperparameters)"
)
1
library(brms)
# 1) Your data
y_obs <- r2.new
x_obs <- r2.observed
set.seed(123)
# 1) Define your grid
grid_int   <- seq(0.0, 1.0, by = 0.2)
grid_sigma <- seq(0.1, 1.0, by = 0.2)
results    <- expand.grid(mu_int = grid_int, sigma = grid_sigma)
results$loss <- NA_real_
# 2) Loss function that only uses base R
loss_for_fast <- function(int_mean, sigma_scale, x_obs, y_obs,
n_draws = 200) {
# draw parameters
alpha_draws <- rnorm(n_draws, mean = int_mean,   sd = 0.15)
beta_draws  <- rnorm(n_draws, mean = 1.0,        sd = 0.15)
sigma_draws <- abs(rt(n_draws, df = 3) * sigma_scale / sqrt(3 / 1))
# Student-t(3,0,s) via scale
# for each draw, simulate y_pred_i = α + β x_i + ε_i
# We'll vectorize: for each draw j, simulate residuals across all species
ysim <- vapply(seq_len(n_draws), function(j) {
alpha   <- alpha_draws[j]
beta    <- beta_draws[j]
sigma_j <- sigma_draws[j]
y_j     <- alpha + beta * x_obs + rnorm(length(x_obs), 0, sigma_j)
return(y_j)
}, numeric(length(x_obs)))
# flatten, compare distributions
ysim_all <- as.vector(ysim)
(mean(ysim_all) - mean(y_obs))^2 +
(quantile(ysim_all, 0.9) - quantile(y_obs, 0.9))^2
}
# 3) Run the fast grid
for (i in seq_len(nrow(results))) {
im <- results$mu_int[i]
ss <- results$sigma[i]
results$loss[i] <- loss_for_fast(im, ss, x_obs, y_obs)
}
best <- results[which.min(results$loss), ]
print(best)
# e.g. mu_int=0.4, sigma=0.3
# 4) Use those best hyperparameters for your priors (then call brm just once)
priors_auto <- c(
prior(normal(best$mu_int, 0.15),   class = "Intercept"),
prior(normal(1,         0.15),   class = "b", coef = "R2_obs"),
prior(student_t(3, 0, best$sigma), class = "sigma")
)
print(priors_auto)
# 5) (Optional) Quick R‐only prior predictive plot
pp_fast <- loss_for_fast(best$mu_int, best$sigma, x_obs, y_obs, n_draws = 500)
# but better to use brm one final time for the real check...
# After your grid‐search, you have:
mu_int      <- best$mu_int    # e.g. 0.4
sigma_scale <- best$sigma     # e.g. 0.3
x_obs       <- x_obs          # your r2.observed vector
y_obs       <- y_obs          # your r2.new vector
n_draws     <- 200            # number of prior‐predictive draws
# 1) Sample from your priors:
set.seed(555)
alpha_draws <- rnorm(n_draws, mean = mu_int, sd = 0.15)
beta_draws  <- rnorm(n_draws, mean = 1.0,    sd = 0.15)
# half‐Student‐t(3) scaled:
t_raw       <- rt(n_draws, df = 3)
sigma_draws <- abs(t_raw) * sigma_scale / sqrt(3/1)
# 2) Simulate y_pred for each draw and each species:
#    result is a n_draws × length(x_obs) matrix
y_sim <- sapply(seq_len(n_draws), function(j) {
a <- alpha_draws[j]
b <- beta_draws[j]
s <- sigma_draws[j]
a + b * x_obs + rnorm(length(x_obs), 0, s)
})
# 3) Plot prior‐predictive cloud vs. observed
matplot(
x    = x_obs,
y    = t(y_sim),          # transpose: columns = draws
type = "p",
pch  = 16,
col  = rgb(0,0,1,0.05),
xlab = "Observed R²",
ylab = "Simulated R² (prior)",
main = "Prior Predictive (auto‐tuned hyperparameters)"
)
1
library(brms)
# 1) Your data
y_obs <- r2.new
x_obs <- r2.observed
set.seed(123)
# 1) Define your grid
grid_int   <- seq(0.0, 1.0, by = 0.2)
grid_sigma <- seq(0.1, 1.0, by = 0.2)
results    <- expand.grid(mu_int = grid_int, sigma = grid_sigma)
results$loss <- NA_real_
# 2) Loss function that only uses base R
loss_for_fast <- function(int_mean, sigma_scale, x_obs, y_obs,
n_draws = 200) {
# draw parameters
alpha_draws <- rnorm(n_draws, mean = int_mean,   sd = 0.15)
beta_draws  <- rnorm(n_draws, mean = 1.0,        sd = 0.15)
sigma_draws <- abs(rt(n_draws, df = 3) * sigma_scale / sqrt(3 / 1))
# Student-t(3,0,s) via scale
# for each draw, simulate y_pred_i = α + β x_i + ε_i
# We'll vectorize: for each draw j, simulate residuals across all species
ysim <- vapply(seq_len(n_draws), function(j) {
alpha   <- alpha_draws[j]
beta    <- beta_draws[j]
sigma_j <- sigma_draws[j]
y_j     <- alpha + beta * x_obs + rnorm(length(x_obs), 0, sigma_j)
return(y_j)
}, numeric(length(x_obs)))
# flatten, compare distributions
ysim_all <- as.vector(ysim)
(mean(ysim_all) - mean(y_obs))^2 +
(quantile(ysim_all, 0.9) - quantile(y_obs, 0.9))^2
}
# 3) Run the fast grid
for (i in seq_len(nrow(results))) {
im <- results$mu_int[i]
ss <- results$sigma[i]
results$loss[i] <- loss_for_fast(im, ss, x_obs, y_obs)
}
best <- results[which.min(results$loss), ]
print(best)
# e.g. mu_int=0.4, sigma=0.3
# 4) Use those best hyperparameters for your priors (then call brm just once)
priors_auto <- c(
prior(normal(best$mu_int, 0.15),   class = "Intercept"),
prior(normal(1,         0.15),   class = "b", coef = "R2_obs"),
prior(student_t(3, 0, best$sigma), class = "sigma")
)
print(priors_auto)
# 5) (Optional) Quick R‐only prior predictive plot
pp_fast <- loss_for_fast(best$mu_int, best$sigma, x_obs, y_obs, n_draws = 500)
# but better to use brm one final time for the real check...
# After your grid‐search, you have:
mu_int      <- best$mu_int    # e.g. 0.4
sigma_scale <- best$sigma     # e.g. 0.3
x_obs       <- x_obs          # your r2.observed vector
y_obs       <- y_obs          # your r2.new vector
n_draws     <- 200            # number of prior‐predictive draws
# 1) Sample from your priors:
set.seed(555)
alpha_draws <- rnorm(n_draws, mean = mu_int, sd = 0.15)
beta_draws  <- rnorm(n_draws, mean = 1.0,    sd = 0.15)
# half‐Student‐t(3) scaled:
t_raw       <- rt(n_draws, df = 3)
sigma_draws <- abs(t_raw) * sigma_scale / sqrt(3/1)
# 2) Simulate y_pred for each draw and each species:
#    result is a n_draws × length(x_obs) matrix
y_sim <- sapply(seq_len(n_draws), function(j) {
a <- alpha_draws[j]
b <- beta_draws[j]
s <- sigma_draws[j]
a + b * x_obs + rnorm(length(x_obs), 0, s)
})
# 3) Plot prior‐predictive cloud vs. observed
matplot(
x    = x_obs,
y    = t(y_sim),          # transpose: columns = draws
type = "p",
pch  = 16,
col  = rgb(0,0,1,0.05),
xlab = "Observed R²",
ylab = "Simulated R² (prior)",
main = "Prior Predictive (auto‐tuned hyperparameters)"
)
# 3) Plot prior‐predictive cloud vs. observed
matplot(
x    = x_obs,
y    = y_sim,          # y_sim is species‐by‐draws
type = "p",
pch  = 16,
col  = rgb(0,0,1,0.05),
xlab = "Observed R²",
ylab = "Simulated R² (prior)",
main = "Prior Predictive (auto‐tuned hyperparameters)"
)
# overlay the identity line and red observed points
abline(0, 1, col = "gray", lty = 2)
points(x_obs, y_obs, col = "red", pch = 19)
library(brms)
library(doParallel)
library(future)
library(ParBayesianOptimization)
library(posterior)
set.seed(1234)
rsq <- read.csv("../results/rsq.csv")
repeats <- read.csv("../results/repeat-results.csv")
dat <- merge(rsq, repeats, by.x = "species", by.y = "species", all.x = T, all.y = T)
results <- read.csv("../results/model-averaging.csv")
i <- 1
# subset data by clade
clade <- unique(results$clade)[i]
if (clade == "All") {
sub <- dat
} else {
sub <- dat[dat$clade == clade, ]
}
dat.vars <- colnames(sub)[grep("^(prop|age)\\.", colnames(sub))]
sub <- na.omit(sub[, c("species", "clade", "rsq", dat.vars)])
# subset data from significant variables
est.vars <- results[results$clade == clade, ]$model
dat.new <- as.data.frame(matrix(NA, nrow(sub), 0))
for (k in 1:length(est.vars)) {
est.var <- est.vars[k]
if (grepl(":", est.var)) {
strsplit(est.var, ":")
dat.cols <- sub[, unlist(strsplit(est.var, ":"))]
dat.new <- cbind(dat.new, apply(dat.cols, 1, prod))
names(dat.new)[k] <- est.var
} else {
dat.cols <- sub[, est.var]
dat.new <- cbind(dat.new, dat.cols)
names(dat.new)[k] <- est.var
}
}
row.names(dat.new) <- sub$species
# rescale data
for (j in 1:length(dat.new)) {
col <- names(dat.new)[j]
dat.new[[col]] <- (dat.new[[col]]-min(dat.new[[col]])) / diff(range(dat.new[[col]]))
}
# simulate R2 using estimated predictors
coefs <- results[results$clade == clade, ]$estimate
dat.new <- as.matrix(dat.new)
r2.new <- dat.new %*% coefs
r2.new <- setNames(as.vector(r2.new), row.names(r2.new))
# match simulated data with observed data
r2.observed <- na.omit(setNames(dat$rsq, dat$species))
int <- intersect(names(r2.new), names(r2.observed))
r2.new <- r2.new[names(r2.new) %in% int]
r2.observed <- r2.observed[names(r2.observed) %in% int]
r2.new <- r2.new + (median(r2.observed) - median(r2.new))
# 1) Prepare data (no change)
df_clade <- data.frame(
R2_obs       = r2.new,
R2_pred_base = r2.observed
)
# 2) Write a non-linear brms formula so we can give hyper-priors
#    We introduce latent parameters alpha_i and beta_i (one per observation),
#    but tie them to a common Normal(mu, sigma) distribution.
bf <- bf(
R2_pred_base ~ alpha + beta * R2_obs,
alpha ~ 1 + (1 |obs),        # random intercept per obs
beta  ~ 1 + (1 |obs),        # random slope per obs
nl = TRUE
)
# 3) Define hyper-priors
priors <- c(
# Hyper-priors on the group-level distributions of alpha and beta
prior(normal(0, 1), class = "b",         nlpar = "alpha"),  # mu_alpha ~ N(0,1)
prior(normal(1, 1), class = "b",         nlpar = "beta"),   # mu_beta  ~ N(1,1)
prior(student_t(3, 0, 1), class = "sd",  nlpar = "alpha"),  # sigma_alpha ~ Student-t(3,0,1)
prior(student_t(3, 0, 1), class = "sd",  nlpar = "beta"),   # sigma_beta  ~ Student-t(3,0,1)
# Residual standard deviation
prior(student_t(3, 0, 1), class = "sigma")
)
# 4) Fit the full hierarchical model in one go
fit_clade <- brm(
formula       = bf,
data          = df_clade,
prior         = priors,
family        = gaussian(),
chains        = 4,
cores         = 4,
iter          = 4000,
control       = list(adapt_delta = 0.95),
seed          = 1234
)
# 1) Prepare data (no change)
df_clade <- data.frame(
R2_obs       = r2.new,
R2_pred_base = r2.observed
)
# 0) Add an observation index
df_clade$obs <- seq_len(nrow(df_clade))
# 1) Prepare data (no change)
df_clade <- data.frame(
R2_obs       = r2.new,
R2_pred_base = r2.observed
)
# 0) Add an observation index
df_clade$obs <- seq_len(nrow(df_clade))
# 1) Non‐linear formula with per‐obs random effects
bf <- bf(
R2_pred_base ~ alpha + beta * R2_obs,
alpha ~ 1 + (1 | obs),
beta  ~ 1 + (1 | obs),
nl = TRUE
)
# 2) Hyper‐priors as before
priors <- c(
prior(normal(0, 1),      class = "b",        nlpar = "alpha"),
prior(normal(1, 1),      class = "b",        nlpar = "beta"),
prior(student_t(3, 0, 1), class = "sd",      nlpar = "alpha"),
prior(student_t(3, 0, 1), class = "sd",      nlpar = "beta"),
prior(student_t(3, 0, 1), class = "sigma")
)
# 3) Fit the model
fit_clade <- brm(
formula   = bf,
data      = df_clade,
prior     = priors,
family    = gaussian(),
chains    = 4,
cores     = 4,
iter      = 4000,
control   = list(adapt_delta = 0.95),
seed      = 1234
)
setwd("~/GitHub/genedensity/scripts")
library(brms)
library(posterior)
source("functions.R")
set.seed(1234)
rsq <- read.csv("../results/rsq.csv")
repeats <- read.csv("../results/repeat-results.csv")
dat <- merge(rsq, repeats, by.x = "species", by.y = "species", all.x = T, all.y = T)
results <- read.csv("../results/model-averaging.csv")
i <- 1
# subset data by clade
clade <- unique(results$clade)[i]
if (clade == "All") {
sub <- dat
} else {
sub <- dat[dat$clade == clade, ]
}
dat.vars <- colnames(sub)[grep("^(prop|age)\\.", colnames(sub))]
sub <- na.omit(sub[, c("species", "clade", "rsq", dat.vars)])
# subset data from significant variables
est.vars <- results[results$clade == clade, ]$model
dat.new <- as.data.frame(matrix(NA, nrow(sub), 0))
for (k in 1:length(est.vars)) {
est.var <- est.vars[k]
if (grepl(":", est.var)) {
strsplit(est.var, ":")
dat.cols <- sub[, unlist(strsplit(est.var, ":"))]
dat.new <- cbind(dat.new, apply(dat.cols, 1, prod))
names(dat.new)[k] <- est.var
} else {
dat.cols <- sub[, est.var]
dat.new <- cbind(dat.new, dat.cols)
names(dat.new)[k] <- est.var
}
}
row.names(dat.new) <- sub$species
# rescale data
for (j in 1:length(dat.new)) {
col <- names(dat.new)[j]
dat.new[[col]] <- (dat.new[[col]]-min(dat.new[[col]])) / diff(range(dat.new[[col]]))
}
# simulate R2 using estimated predictors
coefs <- results[results$clade == clade, ]$estimate
dat.new <- as.matrix(dat.new)
r2.new <- dat.new %*% coefs
r2.new <- setNames(as.vector(r2.new), row.names(r2.new))
# match simulated data with observed data
r2.observed <- na.omit(setNames(dat$rsq, dat$species))
int <- intersect(names(r2.new), names(r2.observed))
r2.new <- r2.new[names(r2.new) %in% int]
r2.observed <- r2.observed[names(r2.observed) %in% int]
# center
r2.observed <- r2.observed - median(r2.observed)
r2.new <- r2.new - median(r2.new)
# 1) Prepare data (no change)
df <- data.frame(
r2.new,
r2.observed,
obs = seq_along(r2.new)
)
?bf()
